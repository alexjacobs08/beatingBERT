# ğŸ“Š Full Benchmark Summary

## What Gets Tested

### Models (5 total)

| Model | Size | Type | Use Cases |
|-------|------|------|-----------|
| BERT-base | 110M | Encoder | Baseline fine-tuning |
| DeBERTa-base | 184M | Encoder | Better encoder |
| Qwen 0.5B | 500M | Decoder | Fastest LLM |
| Qwen 1.5B | 1.5B | Decoder | Balanced LLM |
| Gemma 2B | 2B | Decoder | Best quality LLM |

### Tasks (3 total)

| Task | Type | Size | Difficulty |
|------|------|------|------------|
| **SST-2** | Sentiment (binary) | 67K train | Easy |
| **RTE** | Entailment (binary) | 2.5K train | Hard |
| **MNLI** | NLI (3-way) | 393K train | Medium |

### Modes (4 total)

| Mode | Description | Training Time |
|------|-------------|---------------|
| **Fine-tune** | Full fine-tuning | Minutes-hours |
| **Zero-shot** | No training | Instant |
| **Few-shot** | 5 examples | Instant |
| **LoRA** | Parameter-efficient | Minutes |

## Experiment Matrix

### BERT Models (6 experiments)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ SST2 â”‚ RTE â”‚ MNLI â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ BERT-base   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â”‚ DeBERTa     â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

### LLM Zero-shot (9 experiments)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ SST2 â”‚ RTE â”‚ MNLI â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Qwen 0.5B   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â”‚ Qwen 1.5B   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â”‚ Gemma 2B    â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

### LLM Few-shot (9 experiments)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ SST2 â”‚ RTE â”‚ MNLI â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Qwen 0.5B   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â”‚ Qwen 1.5B   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â”‚ Gemma 2B    â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

### LLM LoRA (6 experiments)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ SST2 â”‚ RTE â”‚ MNLI â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Qwen 0.5B   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â”‚ Qwen 1.5B   â”‚  âœ“   â”‚  âœ“  â”‚  âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

## Total: 30 Experiments

## Estimated Timeline

### Quick Test (~15 min)
```
00:00 - Start
00:03 - BERT test complete
00:06 - Zero-shot complete
00:09 - Few-shot complete
00:15 - LoRA test complete
```

### Full Benchmark (~4-6 hours)

```
Hour 0:00 - Start
Hour 0:00 - BERT fine-tuning begins
â”œâ”€ 00:30 - BERT-base on SST-2 âœ“
â”œâ”€ 00:45 - BERT-base on RTE âœ“
â”œâ”€ 01:15 - BERT-base on MNLI âœ“
â”œâ”€ 01:45 - DeBERTa on SST-2 âœ“
â”œâ”€ 02:00 - DeBERTa on RTE âœ“
â””â”€ 02:30 - DeBERTa on MNLI âœ“

Hour 2:30 - LLM Zero-shot begins
â”œâ”€ 02:40 - Qwen 0.5B complete âœ“
â”œâ”€ 02:55 - Qwen 1.5B complete âœ“
â””â”€ 03:15 - Gemma 2B complete âœ“

Hour 3:15 - LLM Few-shot begins
â”œâ”€ 03:25 - Qwen 0.5B complete âœ“
â”œâ”€ 03:40 - Qwen 1.5B complete âœ“
â””â”€ 04:00 - Gemma 2B complete âœ“

Hour 4:00 - LLM LoRA begins
â”œâ”€ 04:30 - Qwen 0.5B complete âœ“
â”œâ”€ 05:30 - Qwen 1.5B complete âœ“
â””â”€ 06:00 - All done! âœ“
```

**Note**: Times vary based on:
- CPU/GPU speed
- Available RAM
- Dataset size
- Model size

## Expected Outputs

### Results Directory
```
results/
â”œâ”€â”€ benchmark_20251108_100000.log (full log)
â”‚
â”œâ”€â”€ bert_base_uncased_sst2_20251108_100000/
â”‚   â”œâ”€â”€ best_model/
â”‚   â”œâ”€â”€ results.json              â† Accuracy, F1, MCC
â”‚   â”œâ”€â”€ history.json              â† Training curves
â”‚   â””â”€â”€ error_analysis/
â”‚       â”œâ”€â”€ bert_confusion_matrix.png
â”‚       â””â”€â”€ bert_errors.csv
â”‚
â”œâ”€â”€ qwen2_0.5b_zero_shot_sst2_20251108_103000/
â”‚   â”œâ”€â”€ results.json              â† Performance metrics
â”‚   â””â”€â”€ error_analysis/
â”‚
â””â”€â”€ ... (27 more experiment directories)
```

### Analysis Output
After running `analyze_results.py`:
```
analysis/
â”œâ”€â”€ comparison_table.csv
â”œâ”€â”€ accuracy_comparison.png
â”œâ”€â”€ latency_comparison.png
â””â”€â”€ summary_stats.txt
```

## Research Questions Answered

1. **Can small LLMs match BERT on short-text classification?**
   - Compare: BERT fine-tuned vs LLM LoRA

2. **Is few-shot prompting competitive with fine-tuning?**
   - Compare: BERT fine-tuned vs LLM few-shot

3. **How much does model size matter?**
   - Compare: Qwen 0.5B vs 1.5B vs Gemma 2B

4. **Which approach is most efficient?**
   - Compare: Training time, inference latency, memory usage

5. **Where do LLMs struggle vs BERT?**
   - Analyze: Error patterns, task-specific performance

## Success Criteria

### âœ… Benchmark is successful if:
- All 30 experiments complete
- Results saved for each experiment
- No crashes or errors
- Can generate comparison tables

### ğŸ“Š Interesting findings if:
- LLM LoRA within 2% of BERT
- Few-shot competitive with fine-tuning
- Small models (0.5B) surprisingly good
- Clear efficiency trade-offs identified

## Next Steps After Benchmark

1. **Immediate** (5 min)
   ```bash
   uv run python experiments/analyze_results.py
   ```

2. **Deep Dive** (30 min)
   - Open best/worst performing experiments
   - Read error_analysis reports
   - Identify patterns

3. **Report** (1 hour)
   - Create summary table
   - Plot key comparisons
   - Write findings

4. **Iterate** (ongoing)
   - Try different hyperparameters
   - Test more models
   - Explore failure cases

## Quick Commands

```bash
# Run quick test (verify setup)
bash experiments/run_quick_test.sh

# Run full benchmark (3-6 hours)
bash experiments/run_full_benchmark.sh

# Monitor progress
tail -f results/benchmark_*.log

# Analyze when done
uv run python experiments/analyze_results.py

# Check disk space
df -h

# See latest results
ls -lt results/ | head -20
```

---

**Ready to start?** â†’ `bash experiments/run_quick_test.sh`




